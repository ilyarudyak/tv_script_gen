{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_prep import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first. Let's start from data preparation. It's quite minimal here and the main question is how do we construct our batches. \n",
    "\n",
    "We read the text as it is without preprocessing or normalizing and extract all chars from it. So we have a lot of chars and we organize them using a dictionary where each char has an index. We also encode text as a list of numbers where each number is an index of a char in the mentioned dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('9', 0), ('*', 1), ('z', 2), ('r', 3), (\"'\", 4)]\n",
      "[('c', 78), ('m', 79), ('!', 80), ('0', 81), ('&', 82)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "print(list(char2int.items())[:5]), print(list(char2int.items())[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([70, 10, 60, 21, 24, 57,  3, 52, 76, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C', 'h', 'a', 'p', 't', 'e', 'r', ' ', '1', '\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[int2char[i] for i in encoded[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step - we do one-hot encoding. Function mechanics is a bit involved but the result is quite clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[70, 10, 60, 21, 24],\n",
       "        [57,  3, 52, 76, 64]]), (2, 5))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = encoded[:10].reshape(2, 5)\n",
    "arr, arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = one_hot_encode(arr=arr, n_labels=len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5, 83)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot[0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot[0, 0, 22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's the most interesting part - batch construction. We have 2 new parameters: `batch_size=128` and `seq_length=100`. So we're going to reshape `encoded` into `128` rows and then use a sliding window of size `100` to iterate over it. \n",
    "\n",
    "First question - why don't we construct a batch using sequntial values of `encoded`? Well probably because it doesn't really matter which way to construct. This is char RNN after all. We need to predict next char based on some previos ones and `seq_length` is quite high to make all rows of a batch completely independent. But this is just a hypothesis. In fact we have stateful and stateless `RNNs` - see *Hands-On Machine Leaning, ch. 16*. And in stateful `RNNs` the question how to build a batch does matter.\n",
    "\n",
    "Let's use much less values of parameters for illustration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_length = 3, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = encoded\n",
    "n_batches = int(arr.size / (batch_size * seq_length))\n",
    "arr = arr[:(n_batches * batch_size * seq_length)]\n",
    "arr = arr.reshape((batch_size, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 661740)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[70, 10, 60, 21, 24, 57,  3, 52, 76, 64],\n",
       "       [12, 31, 73, 57, 78, 24, 35, 64, 64,  8],\n",
       "       [72, 57, 52, 62, 34, 57,  3, 64, 60, 49]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[:, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should our batches look like? It's important that a sliding window goes in `seq_length` steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[70, 10, 60, 21, 24],\n",
       "       [12, 31, 73, 57, 78],\n",
       "       [72, 57, 52, 62, 34]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[:, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[57,  3, 52, 76, 64],\n",
       "       [24, 35, 64, 64,  8],\n",
       "       [57,  3, 64, 60, 49]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[:, 5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_generator = get_batches(arr=encoded, batch_size=3, seq_length=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[70, 10, 60, 21, 24],\n",
       "       [12, 31, 73, 57, 78],\n",
       "       [72, 57, 52, 62, 34]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(batch_generator)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[57,  3, 52, 76, 64],\n",
       "       [24, 35, 64, 64,  8],\n",
       "       [57,  3, 64, 60, 49]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, _ = next(batch_generator)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last question - how do we compute `y`? It should be shifted by one position - we predict next character in a sequence after all. So if first row is `[22, 52, 26,  1, 80, 43, 16, 28,  2, 19, ...]` then the first row of `x` is `[22, 52, 26,  1, 80]`  and the first row of `y` is `[52, 26, 1, 80, 43]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 60, 21, 24, 57],\n",
       "       [31, 73, 57, 78, 24],\n",
       "       [57, 52, 62, 34, 57]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[:, 1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 60, 21, 24, 57],\n",
       "       [31, 73, 57, 78, 24],\n",
       "       [57, 52, 62, 34, 57]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next question - let's decompose our model bit-by-bit and simulate forward pass. There're a lot of questions out there - where do we use `seq_length` in `LSTM`, what is output of `LSTM`, do we use `softmax` or not, can we handle sequences of variable length and so on.\n",
    "\n",
    "First question - what exactly do we need to construct `LSTM`? Let's start with `LSTMCell`. All the gates in `LSTMCell` are basically linear layers with some activation. So we have matrices of weights that depend on 2 parameters: `input_size` and `hidden_size`. And these are exactly parameters that we need to instantiate `LSTMCell`.\n",
    "\n",
    "Now to instantiate `LSTM` we need only one additional parameter - `n_layers`. In our case `n_layers=2` and that means that we have 2 layers of LSTM stacked together. So here's the question - how do `LSTM` handle unrolling? How does it know `seq_length`?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 83]), torch.Size([3, 5]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, seq_length, n_chars = 3, 5, 83\n",
    "batch_gen = get_tensor_batches(encoded, batch_size, seq_length, n_chars)\n",
    "x, y = next(batch_gen)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 60, 21, 24, 57],\n",
       "        [31, 73, 57, 78, 24],\n",
       "        [57, 52, 62, 34, 57]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple `LSTM` with just `1` layer. Let's use `bias=False` to simplify computations.\n",
    "\n",
    "What does `batch_first=True` mean? From documentation: *If `True`, then the input and output tensors are provided as `(batch, seq, feature)`.* And this is indeed the case: `x.shape[0]` is in fact the `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "lstm_cell = nn.LSTMCell(input_size=len(chars),\n",
    "                       hidden_size=2,\n",
    "                       bias=False)\n",
    "torch.manual_seed(0)\n",
    "lstm = nn.LSTM(input_size=len(chars), \n",
    "               hidden_size=2, \n",
    "               num_layers=1,\n",
    "               dropout=0, \n",
    "               batch_first=True,\n",
    "               bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 83]) torch.Size([8, 2])\n",
      "torch.Size([8, 83]) torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "# shapes should be (4 * hidden_size, input_size)\n",
    "# and (4 * hidden_size, hidden_size)\n",
    "print(lstm_cell.weight_ih.shape, lstm_cell.weight_hh.shape)\n",
    "print(lstm.weight_ih_l0.shape, lstm.weight_hh_l0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next question - how should we initialize our hidden state? It turns out that most popular solution is just set it to `0` (again, see an alternative in *Hands-On Machine Leaning, ch. 16*). And it seems this option is now available by default, from documentation: *If `(h_0, c_0)` is not provided, both `h_0` and `c_0` default to zero.* This was not alwais the case: see [here](https://github.com/pytorch/pytorch/issues/434).\n",
    "\n",
    "That's easy to check directly: below we do computations with default params and with `(h0, c0)` set to `0` - result is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### computations of `lstm_cell`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's try to reproduce computations of the `LSTMCell`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 83])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 83])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = x[:, 0, :]\n",
    "x0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check that we don't have bias\n",
    "lstm_cell.bias_ih, lstm_cell.bias_hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 83]),\n",
       " torch.Size([2, 83]),\n",
       " torch.Size([2, 83]),\n",
       " torch.Size([2, 83]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wii, Wif, Wig, Wio = lstm_cell.weight_ih[0:2, :], lstm_cell.weight_ih[2:4, :], \\\n",
    "                     lstm_cell.weight_ih[4:6, :], lstm_cell.weight_ih[6:8, :]\n",
    "Wii.shape, Wif.shape, Wig.shape, Wio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2]) torch.Size([3, 2]) torch.Size([3, 2])\n",
      "torch.Size([3, 2]) torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    i = torch.sigmoid(torch.mm(x0, Wii.t()))\n",
    "    g = torch.tanh(torch.mm(x0, Wig.t()))\n",
    "    o = torch.sigmoid(torch.mm(x0, Wio.t()))\n",
    "    print(i.shape, g.shape, o.shape)\n",
    "\n",
    "    c_man = i * g\n",
    "    h_man = o * torch.tanh(c_man)\n",
    "    print(h_man.shape, c_man.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0747,  0.1230],\n",
       "        [-0.0605, -0.1216],\n",
       "        [ 0.1025,  0.0474]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1182,  0.2956],\n",
       "        [-0.1354, -0.1866],\n",
       "        [ 0.1601,  0.1251]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]), tensor([[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0, c0 = torch.zeros(3, 2), torch.zeros(3, 2)\n",
    "h0, c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    h_comp, c_comp = lstm_cell(x0, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0747,  0.1230],\n",
       "        [-0.0605, -0.1216],\n",
       "        [ 0.1025,  0.0474]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1182,  0.2956],\n",
       "        [-0.1354, -0.1866],\n",
       "        [ 0.1601,  0.1251]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### computations of `lstm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now is the **hard** part - computations of the `LSTM`. Probably we may use `LSTMCell` and a loop rather than do all computations manually. For convenience let's define all inputs and models one more time.\n",
    "\n",
    "First we create `LSTM` and `LSTMCell`. To use `LSTMCell` to reproduce behavior of `LSTM` we need to replace weights in `LSTMCell` using weights of `LSTM`. And we're ready to unroll the loop:  we use length of a sequence to perform computations. The loop is the same as we can find in `pytorch` [docs](https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTMCell). Initially (h, c) are 0s and then we just write:\n",
    "`h, c = lstm_cell(x, (h, c))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 83]), torch.Size([3, 5]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(batch_gen)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "lstm = nn.LSTM(input_size=len(chars), \n",
    "               hidden_size=2, \n",
    "               num_layers=1,\n",
    "               dropout=0, \n",
    "               batch_first=True,\n",
    "               bias=False)\n",
    "torch.manual_seed(0)\n",
    "lstm_cell = nn.LSTMCell(input_size=len(chars),\n",
    "                       hidden_size=2,\n",
    "                       bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cell.weight_ih = lstm.weight_ih_l0\n",
    "lstm_cell.weight_hh = lstm.weight_hh_l0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0053,  0.3793, -0.5820, -0.5204, -0.2723], grad_fn=<SliceBackward>)\n",
      "tensor([-0.0053,  0.3793, -0.5820, -0.5204, -0.2723], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(lstm_cell.weight_ih[0, :5])\n",
    "print(lstm.weight_ih_l0[0, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1121,  0.1053],\n",
      "        [ 0.0386,  0.2719],\n",
      "        [-0.0643, -0.0012]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.2603,  0.1606],\n",
      "        [ 0.0702,  0.4512],\n",
      "        [-0.1267, -0.0020]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "h_cell, c_cell = torch.zeros(3, 2), torch.zeros(3, 2)\n",
    "for i in range(x.shape[1]):\n",
    "    xi = x[:, i, :]\n",
    "    h_cell, c_cell = lstm_cell(xi, (h_cell, c_cell))\n",
    "print(h_cell)\n",
    "print(c_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1121,  0.1053],\n",
      "         [ 0.0386,  0.2719],\n",
      "         [-0.0643, -0.0012]]], grad_fn=<StackBackward>)\n",
      "tensor([[[ 0.2603,  0.1606],\n",
      "         [ 0.0702,  0.4512],\n",
      "         [-0.1267, -0.0020]]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "output, (h, c) = lstm(x)\n",
    "print(h)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check `output`. From documentation: *tensor containing the output features (h_t) from the last layer of the LSTM, for each t*. First of all we see that `output[:, 4, :]` is in fact equal to `h` in above computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 2])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1121,  0.1053],\n",
       "        [ 0.0386,  0.2719],\n",
       "        [-0.0643, -0.0012]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[:, 4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0578, 0.0923]) tensor([0.0578, 0.0923], requires_grad=True)\n",
      "tensor([0.1041, 0.1083]) tensor([0.1041, 0.1083], requires_grad=True)\n",
      "tensor([ 0.0826, -0.0348]) tensor([ 0.0826, -0.0348], requires_grad=True)\n",
      "tensor([-0.0163, -0.0010]) tensor([-0.0163, -0.0010], requires_grad=True)\n",
      "tensor([0.1121, 0.1053]) tensor([0.1121, 0.1053], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    h_cell, c_cell = torch.zeros(3, 2), torch.zeros(3, 2)\n",
    "    for i in range(x.shape[1]):\n",
    "        xi = x[:, i, :]\n",
    "        h_cell, c_cell = lstm_cell(xi, (h_cell, c_cell))\n",
    "        print(h_cell[0, :], output[:, i, :][0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it for this char RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
